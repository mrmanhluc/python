{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import codecs as cd\n",
    "import glob\n",
    "import xlsxwriter\n",
    "import os\n",
    "from dateutil import parser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Common Function\n",
    "\n",
    "#### 1. Get_Columns_Metrics\n",
    "\n",
    "Input Data = DataFrame\n",
    "\n",
    "Output : \n",
    "\n",
    "| Columns_name       | Tên columns            |\n",
    "|--------------------|------------------------|\n",
    "| Values_Range       | Giá trị Min : Max      |\n",
    "| Null_nums          | Số lượng giá trị Nulls |\n",
    "| Rows_nums          | Tổng số rows           |\n",
    "| Unique_values      | Giá trị unique         |\n",
    "| Unique_nums        | Tổng số lượng unique   |\n",
    "| Category_types     | Loại Category type     |\n",
    "| Unique_values_rate | Tỉ lệ unique           |\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_number(str):\n",
    "    '''\n",
    "        Input : String\n",
    "        Output : Float. If Null values will be return None\n",
    "    '''\n",
    "    temp = None\n",
    "    try:\n",
    "        temp = float(str)\n",
    "    except ValueError:\n",
    "        return None\n",
    "    return temp\n",
    "\n",
    "def to_date(column):\n",
    "    '''\n",
    "        Input : Pandas Serial\n",
    "    '''\n",
    "    tmp_date = []\n",
    "    for x in column:\n",
    "        try:\n",
    "            tmp_date.append(parse(str(x)))\n",
    "        except ValueError:\n",
    "            tmp_date.append(None)\n",
    "    return tmp_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_date(in_data):\n",
    "    # Kiểm tra có phải là số không\n",
    "    re.match(r'^[0-9]+$', txt1)\n",
    "\n",
    "    # Kiểm tra str có phải dạng datetime không\n",
    "    pattern1 = r'^\\d\\d\\d\\d/\\d\\d/\\d\\d$'\n",
    "    pattern2 = r'^\\d\\d\\/\\d\\d/\\d\\d$'\n",
    "    pattern3 = r'^\\d\\d\\d\\d\\/\\d\\d\\/\\d\\d \\d\\d\\:\\d\\d$'\n",
    "    pattern4 = r'^\\d\\d\\/\\d\\d\\/\\d\\d \\d\\d\\:\\d\\d$'\n",
    "    pattern = [pattern1, pattern2, pattern3, pattern4]\n",
    "\n",
    "    for pt in parterm:\n",
    "        if re.match(pt, in_data) is not None:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def is_num(in_data):\n",
    "    return in_data.replace('.','',1).isdigit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_Columns_Metrics(In_data):\n",
    "    '''\n",
    "    Return list of all file with extendsion is csv in folder\n",
    "    \n",
    "    WARNING : - Chua check datatype. Dinh dang Datime. Gia tri max min cua datetime\n",
    "    1. Datetime => Dua ra khoang tu nam nao toi nam nao => Dua ra thong ke du lieu moi nam chiem bao nhieu %\n",
    "    2. String => Dua ra duoc do dai cua string => Dua ra khoang max min ki tu\n",
    "    3. Int => Dua ra gia tri Max Min\n",
    "    \n",
    "    :param fileType: DataFrame\n",
    "    :return: DataFrame and add 3 columns with Category, Size and Sample Data\n",
    "    '''\n",
    "    # Validate input\n",
    "    if isinstance(In_data, pd.DataFrame) == False:\n",
    "        print('Input values is not Dataframe')\n",
    "        return\n",
    "    \n",
    "    tmp_data = In_data.copy()\n",
    "    \n",
    "    rows_nums = len(contentFile)\n",
    "    \n",
    "    # IF distinct value numbers > 10% then that values is continues data type\n",
    "    CATEGORY_DEFINITION = 0.1\n",
    "    TOP_UNIQUE = 10\n",
    "    PRECISION = 2\n",
    "    \n",
    "    unique_values_values = []\n",
    "    \n",
    "    cols_name = []\n",
    "\n",
    "    # Duyet toan bo columns name cua table \n",
    "    for col_name in tmp_data.columns:\n",
    "        # Get each columns name\n",
    "        cols_name.append(col_name)\n",
    "        \n",
    "        # Get unique values\n",
    "        unique_values = tmp_data[str(col_name)].unique()\n",
    "        \n",
    "        # Get numbers of unique values\n",
    "        unique_nums = len(unique_values)\n",
    "        \n",
    "        # Category Type\n",
    "        category_type = ''\n",
    "        \n",
    "        # Get rate of each unique values\n",
    "        unique_values_rate = {}\n",
    "        \n",
    "        # Get Null values\n",
    "        null_nums = tmp_data[col_name].isnull().sum()\n",
    "        \n",
    "        # Get Min, Max values\n",
    "        min_value, max_value = None, None\n",
    "        try:\n",
    "            min_value = float(tmp_data.dropna()[col_name].min())\n",
    "            max_value = float(tmp_data.dropna()[col_name].max())\n",
    "        except ValueError:\n",
    "            min_value, max_value = None, None\n",
    "        values_range = str([min_value, max_value])\n",
    "        \n",
    "        if unique_nums > (rows_nums * CATEGORY_DEFINITION):\n",
    "            unique_values = None\n",
    "            unique_nums = None\n",
    "            category_type = 'Continuous'\n",
    "        else:\n",
    "            if unique_nums == 2:\n",
    "                category_type = 'Binary'\n",
    "            else:\n",
    "                category_type = 'Ordinal'\n",
    "            \n",
    "            # Dat try catch cho viec /0\n",
    "            try:\n",
    "                unique_values_rate = str(pd.Series(contentFile.groupby([col_name])[col_name].count()/rows_nums *100).round(decimals = PRECISION)[:TOP_UNIQUE].to_dict())\n",
    "            except ValueError:\n",
    "                unique_values_rate = None\n",
    "        \n",
    "        # Add toan bo gia tri vao List theo cau truc [<Gia tri Unique>, <So luong gia tri unique>, <Data Category>]\n",
    "        add_rows = {\n",
    "                    'Values_Range' : values_range,\n",
    "                    'Null_nums' : null_nums,\n",
    "                    'Rows_nums' : rows_nums,\n",
    "                    'Unique_values' : str(unique_values), \n",
    "                    'Unique_nums' : unique_nums, \n",
    "                    'Category_types' : category_type,\n",
    "                    'Unique_values_rate' : unique_values_rate\n",
    "                    }\n",
    "        \n",
    "        unique_values_values.append(add_rows)\n",
    "\n",
    "    # Return Table with all values\n",
    "    return pd.DataFrame(unique_values_values, index = cols_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Class Get Info of File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class File_info:\n",
    "    def get_fileName(url):\n",
    "        return url[url.rfind('\\\\') + 1 :]\n",
    "\n",
    "    def get_Extension(url):\n",
    "        return url[url.rfind('.') + 1 :]\n",
    "    \n",
    "    def get_FileSize(url):\n",
    "        return os.path.getsize(url)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Class Read Content from CSV File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Read_file:\n",
    "    '''\n",
    "    Read context of file\n",
    "    '''\n",
    "    def __init__(self, in_url):\n",
    "        self.url = in_url\n",
    "        \n",
    "        # Check dieu kien file co phai la csv hay khong\n",
    "        \n",
    "        # Get file name from in_url\n",
    "        self.fileName = File_info.get_fileName(in_url)\n",
    "        \n",
    "        # Get file size\n",
    "        self.file_size = File_info.get_FileSize(in_url)\n",
    "        \n",
    "        # Get extendion of file from in url\n",
    "        self.fileExtention = File_info.get_Extension(in_url)\n",
    "        \n",
    "        self.file_context = ''\n",
    "        self.file_columns = ''\n",
    "        \n",
    "        if self.fileExtention == 'csv':\n",
    "            self.file_context = self.__read_csv()\n",
    "            self.file_columns = self.file_context.columns\n",
    "        \n",
    "    def __read_csv(self):\n",
    "        with cd.open(self.url, \"r\", \"Shift-JIS\", \"ignore\") as csv_file:\n",
    "            df = pd.read_table(csv_file, sep=',')\n",
    "        return df\n",
    "    \n",
    "    def getLst_columns(self):\n",
    "        return self.file_columns\n",
    "    \n",
    "    def get_context(self):\n",
    "        return self.file_context\n",
    "    \n",
    "    def get_fileName(self):\n",
    "        return self.fileName\n",
    "    \n",
    "    def get_fileExtension(self):\n",
    "        return self.fileExtention\n",
    "\n",
    "    def get_file_Columns(self):\n",
    "        return self.file_columns\n",
    "    \n",
    "    def get_file_size(self):\n",
    "        return self.file_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Class Read all file path in folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Read_folder:\n",
    "    '''\n",
    "    Read data from csv file\n",
    "    '''\n",
    "    def __init__(self, in_folder_url):\n",
    "        self.folder_url = in_folder_url\n",
    "        self.csv_url = self.__read_csv_url()\n",
    "        self.csv_name = self.__read_csv_name()\n",
    "\n",
    "    def __read_csv_url(self):\n",
    "        return glob.glob(self.folder_url+'\\\\*.csv')\n",
    "    \n",
    "    def __read_csv_name(self):\n",
    "        csv_name = []\n",
    "        for url in self.csv_url:\n",
    "            csv_name.append(File_info.get_fileName(url))\n",
    "        return csv_name\n",
    "    \n",
    "    def getLst_csv_url(self):\n",
    "        '''\n",
    "        Return list of all file with extendsion is csv in folder\n",
    "        :param fileType: No\n",
    "        :return: URL with file name\n",
    "        '''        \n",
    "        return self.csv_url\n",
    "    \n",
    "    def getLst_csv_name(self):\n",
    "        return self.csv_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Class write output to Excel file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Write_output:\n",
    "    '''\n",
    "    Read file in input folder\n",
    "    '''\n",
    "    def __init__(self, out_url, sheet_name):\n",
    "        self.URL = out_url\n",
    "        self.sheet_name = sheet_name.copy()\n",
    "\n",
    "    def write_into_sheet(self, data_input, in_sheet_name):\n",
    "        data_input.to_excel(self.URL, sheet_name=in_sheet_name)\n",
    "\n",
    "    def create_excel(self, dashboard = False):\n",
    "        '''\n",
    "        Create Excel file with sheet name is file in folder\n",
    "        :return: Excel File\n",
    "        '''\n",
    "        tmp_sheet_name = self.sheet_name\n",
    "        \n",
    "        if dashboard:\n",
    "            tmp_sheet_name.append('Dashboard')\n",
    "            \n",
    "        workbook = xlsxwriter.Workbook(self.URL)\n",
    "        for s_name in tmp_sheet_name:\n",
    "            workbook.add_worksheet(s_name)\n",
    "        workbook.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Get infor of file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['D:\\\\01_Project\\\\Lixil Analysis\\\\input\\\\data\\\\catelog.csv', 'D:\\\\01_Project\\\\Lixil Analysis\\\\input\\\\data\\\\faq.csv', 'D:\\\\01_Project\\\\Lixil Analysis\\\\input\\\\data\\\\reception.csv']\n",
      "['catelog.csv', 'faq.csv', 'reception.csv']\n"
     ]
    }
   ],
   "source": [
    "# Step 1 : Khởi tạo biến read input\n",
    "URL = r'D:\\01_Project\\Lixil Analysis\\input\\data'\n",
    "read_input = Read_folder(URL)\n",
    "\n",
    "# Toàn bộ URL của file csv\n",
    "print(read_input.getLst_csv_url())\n",
    "\n",
    "# Toàn bộ file name của file csv trong folder\n",
    "print(read_input.getLst_csv_name())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Read info of each file in folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lucnm\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:2910: DtypeWarning: Columns (0,7,8,9,17,34,36) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "C:\\Users\\lucnm\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:2910: DtypeWarning: Columns (14,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,94,95,96,97,98,99,100,101,102,103,104,105,106,107,108,109,110,111,112,113,114,115,116,117,118,119,120,121,122,123,124,125,126,127,128,129,130,131,132) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n"
     ]
    }
   ],
   "source": [
    "# Đọc nội dung lần lượt các file trên\n",
    "finding_data = []\n",
    "file_name = []\n",
    "file_columns = []\n",
    "file_size = []\n",
    "\n",
    "for file_url in read_input.getLst_csv_url():\n",
    "    read_file = Read_file(file_url)\n",
    "    file_name.append(read_file.get_fileName())\n",
    "    file_columns.append(read_file.get_file_Columns())\n",
    "    file_size.append(read_file.get_file_size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_num = []\n",
    "for i in file_columns:\n",
    "    col_num.append(len(i))\n",
    "\n",
    "Dashboard = pd.DataFrame({\n",
    "    'File Name': file_name,\n",
    "    'Columns numbers': col_num,\n",
    "    'File Size': file_size\n",
    "})\n",
    "\n",
    "write = Write_output('D:\\Report.xlsx', file_name)\n",
    "write.create_excel(True)\n",
    "\n",
    "## Write Dashboard into Dashboard sheet\n",
    "write.write_into_sheet(Dashboard, 'dashboard')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## 3. Create Excel which each sheet name is file name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 1: Lay Url file dau tien\n",
    "url_file = read_input.getLst_csv_url()[0]\n",
    "\n",
    "# Step 2 : Doc noi dung file trne\n",
    "readFile = Read_file(url_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Step 3 : Get Content\n",
    "contentFile = readFile.get_context()\n",
    "\n",
    "# dataFr = pd.concat([dataFr, dataFr1], axis=1)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "data_metrics = {}\n",
    "\n",
    "for url in read_input.getLst_csv_url():\n",
    "    file = Read_file(url)\n",
    "    file_context = file.get_context()\n",
    "    file_context_info = get_Columns_Metrics(file_context)\n",
    "    data_metrics[File_info.get_fileName(url)] = file_context_info\n",
    "    file_context_info.drop(file_context_info.index, inplace=True)\n",
    "   # break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TEST REGION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lucnm\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:2910: DtypeWarning: Columns (0,7,8,9,17,34,36) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n"
     ]
    }
   ],
   "source": [
    "#Step 1: Lay Url file dau tien\n",
    "url_file = read_input.getLst_csv_url()[1]\n",
    "\n",
    "# Step 2 : Doc noi dung file trne\n",
    "readFile = Read_file(url_file)\n",
    "\n",
    "# Step 3 : Get Content\n",
    "contentFile = readFile.get_context()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "from dateutil import parser\n",
    "\n",
    "def if_nan(in_str):\n",
    "    if in_str is None:\n",
    "        return '0000/00/00 00:00'\n",
    "    else:\n",
    "        return str(in_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datetime.datetime(2014, 6, 19, 20, 48)"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for x in temp1['作成日']:\n",
    "    try:\n",
    "        temp.append(parse(str(x)))\n",
    "    except ValueError:\n",
    "        temp.append(None)\n",
    "\n",
    "temp[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp0 = to_date(temp1['作成日'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    35\n",
      "Name: FAQID, dtype: object\n",
      "0    [〔トステム〕PRO-SE･L3]PRO-SE･L3のTS、TL網戸の代替品としてTLCで製...\n",
      "Name: 質問, dtype: object\n",
      "0    PRO-SE･L3に使用している網戸はデュオのTS、TL網戸と同じ網戸を使用しています。カタ...\n",
      "Name: 回答, dtype: object\n",
      "292    どうしても必要な場合は、下記参照。\\r\\n「ATL-243」のスライダーピン無にしたものを製...\n",
      "Name: 特記事項, dtype: object\n",
      "0    [〔トステム〕PRO-SE･L3]PRO-SE･L3のTS、TL網戸の代替品としてTLCで製...\n",
      "Name: 質問（HTML）, dtype: object\n",
      "0    <FONT face=\"ＭＳ Ｐゴシック\" size=3>PRO-SE･L3に使用している網...\n",
      "Name: 回答（HTML）, dtype: object\n",
      "292    <P>どうしても必要な場合は、下記参照。</P>\\r\\n<P>「ATL-243」のスライダー...\n",
      "Name: 特記事項（HTML）, dtype: object\n",
      "74361    A17213\n",
      "Name: 質問（携帯）, dtype: object\n",
      "74361    【EW】EW機能一覧表（171114、改訂31） .xls\n",
      "Name: 回答（携帯）, dtype: object\n",
      "74361    INAX\n",
      "Name: 質問（携帯HTML）, dtype: object\n",
      "103610    2.0\n",
      "Name: 回答（携帯HTML）, dtype: float64\n",
      "0    ビル・マンション・店舗\\サッシ\\〔トステム〕PRO-SE･L3\\網戸･付属部材等その他\n",
      "Name: カテゴリ, dtype: object\n",
      "0    管理者画面|利用者画面\n",
      "Name: 参照区分, dtype: object\n",
      "0    1.0\n",
      "Name: 参照レベル, dtype: float64\n",
      "0    1.0\n",
      "Name: 新着表示, dtype: float64\n",
      "0    1.0\n",
      "Name: ランキング表示, dtype: float64\n",
      "0    1.0\n",
      "Name: アンケート表示, dtype: float64\n",
      "0    0\n",
      "Name: 優先度, dtype: object\n",
      "4    ＦＩＸ  はめころし  平面アール\n",
      "Name: キーワード（手動）, dtype: object\n",
      "9    8|67\n",
      "Name: 関連FAQ, dtype: object\n",
      "0    1970/1/1 0:00\n",
      "Name: 公開開始日, dtype: object\n",
      "0    9999/12/31 23:59\n",
      "Name: 公開終了日, dtype: object\n",
      "0    2014/6/19 20:48\n",
      "Name: 作成日, dtype: object\n",
      "0    admin\n",
      "Name: 作成者ID, dtype: object\n",
      "0    2014/6/19 20:48\n",
      "Name: 最終更新日, dtype: object\n",
      "0    admin\n",
      "Name: 更新者ID, dtype: object\n",
      "22    fr001フロアヒンジ調整５００番台.pdf\n",
      "Name: 添付ファイル, dtype: object\n",
      "0    トステム\n",
      "Name: 属性:ブランド, dtype: object\n",
      "0    現行販売商品\n",
      "Name: 属性:販売区分, dtype: object\n",
      "0    互換性\n",
      "Name: 属性:目的, dtype: object\n",
      "0    ビル・マンション・店舗\n",
      "Name: 属性:商品カテゴリ１, dtype: object\n",
      "1434    玄関ドア\n",
      "Name: 属性:商品カテゴリ２, dtype: object\n",
      "5854    特注\n",
      "Name: 属性:商品カテゴリ３, dtype: object\n",
      "5130    旧客相ＦＡＱ\n",
      "Name: 属性:商品カテゴリ４, dtype: object\n",
      "5130    19218\n",
      "Name: 属性:住設FAQNo, dtype: object\n",
      "5130    商品特定|互換性\n",
      "Name: 属性:目的属性名, dtype: object\n",
      "67559    削除する\n",
      "Name: 属性:削除用データ, dtype: object\n",
      "Series([], Name: 属性:削除用キー, dtype: float64)\n"
     ]
    }
   ],
   "source": [
    "temp0 = contentFile.copy()\n",
    "\n",
    "# Function check data type\n",
    "def get_dataType (str):\n",
    "    \n",
    "# Check toan bo column va kieu data\n",
    "\n",
    "# Duyet toan bo column\n",
    "\n",
    "for col in temp0.columns:\n",
    "    print(temp0[col].dropna()[0:1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Update Region"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_Datetime(str):\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_Columns_Metrics_Update(In_data):\n",
    "    '''\n",
    "    Return list of all file with extendsion is csv in folder\n",
    "    \n",
    "    WARNING : - Chua check datatype. Dinh dang Datime. Gia tri max min cua datetime\n",
    "    1. Datetime => Dua ra khoang tu nam nao toi nam nao => Dua ra thong ke du lieu moi nam chiem bao nhieu %\n",
    "    2. String => Dua ra duoc do dai cua string => Dua ra khoang max min ki tu\n",
    "    3. Int => Dua ra gia tri Max Min\n",
    "    \n",
    "    :param fileType: DataFrame\n",
    "    :return: DataFrame and add 3 columns with Category, Size and Sample Data\n",
    "    '''\n",
    "    # Validate input\n",
    "    if isinstance(In_data, pd.DataFrame) == False:\n",
    "        print('Input values is not Dataframe')\n",
    "        return\n",
    "    \n",
    "    tmp_data = In_data.copy()\n",
    "    \n",
    "    rows_nums = len(contentFile)\n",
    "    \n",
    "    # IF distinct value numbers > 10% then that values is continues data type\n",
    "    CATEGORY_DEFINITION = 0.1\n",
    "    TOP_UNIQUE = 10\n",
    "    PRECISION = 2\n",
    "    \n",
    "    unique_values_values = []\n",
    "    \n",
    "    cols_name = []\n",
    "\n",
    "    # Duyet toan bo columns name cua table \n",
    "    for col_name in tmp_data.columns:\n",
    "        # Get each columns name\n",
    "        cols_name.append(col_name)\n",
    "        \n",
    "        # Get unique values\n",
    "        unique_values = tmp_data[str(col_name)].unique()\n",
    "        \n",
    "        # Get numbers of unique values\n",
    "        unique_nums = len(unique_values)\n",
    "        \n",
    "        # Category Type\n",
    "        category_type = ''\n",
    "        \n",
    "        # Get rate of each unique values\n",
    "        unique_values_rate = {}\n",
    "        \n",
    "        # Get Null values\n",
    "        null_nums = tmp_data[col_name].isnull().sum()\n",
    "        \n",
    "        # Get Min, Max values\n",
    "        min_value, max_value = None, None\n",
    "        try:\n",
    "            min_value = float(tmp_data.dropna()[col_name].min())\n",
    "            max_value = float(tmp_data.dropna()[col_name].max())\n",
    "        except ValueError:\n",
    "            min_value, max_value = None, None\n",
    "        values_range = str([min_value, max_value])\n",
    "        \n",
    "        if unique_nums > (rows_nums * CATEGORY_DEFINITION):\n",
    "            unique_values = None\n",
    "            unique_nums = None\n",
    "            category_type = 'Continuous'\n",
    "        else:\n",
    "            if unique_nums == 2:\n",
    "                category_type = 'Binary'\n",
    "            else:\n",
    "                category_type = 'Ordinal'\n",
    "            \n",
    "            # Dat try catch cho viec /0\n",
    "            try:\n",
    "                unique_values_rate = str(pd.Series(contentFile.groupby([col_name])[col_name].count()/rows_nums *100).round(decimals = PRECISION)[:TOP_UNIQUE].to_dict())\n",
    "            except ValueError:\n",
    "                unique_values_rate = None\n",
    "        \n",
    "        # Add toan bo gia tri vao List theo cau truc [<Gia tri Unique>, <So luong gia tri unique>, <Data Category>]\n",
    "        add_rows = {\n",
    "                    'Values_Range' : values_range,\n",
    "                    'Null_nums' : null_nums,\n",
    "                    'Rows_nums' : rows_nums,\n",
    "                    'Unique_values' : str(unique_values), \n",
    "                    'Unique_nums' : unique_nums, \n",
    "                    'Category_types' : category_type,\n",
    "                    'Unique_values_rate' : unique_values_rate\n",
    "                    }\n",
    "        \n",
    "        unique_values_values.append(add_rows)\n",
    "\n",
    "    # Return Table with all values\n",
    "    return pd.DataFrame(unique_values_values, index = cols_name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
